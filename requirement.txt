# LangChain Core
langchain
langchain-core

# OpenAI Integration
langchain-openai
openai

# Anthropic Integration
langchain-anthropic

# Google Gemini (PaLM) Integration
langchain-google-genai
google-generativeai

# Hugging Face Integration
langchain-huggingface
transformers
huggingface-hub

# Environment Variable Management
python-dotenv

# Machine Learning Utilities
numpy
scikit-learn


Run SDXL locally using diffusers

Use Colab notebooks with GPU runtime (Free or Pro)

Use Ollama or LM Studio to run LLMs on CPU/M1/M2, lets talk one by one first -  transformers + AutoGPTQ what specks need